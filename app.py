"""
RAG Chat Application
Streamlit ile PDF/CSV yÃ¼kleyip soru-cevap yapabileceÄŸiniz arayÃ¼z
"""

import streamlit as st
import os
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

# Kendi modÃ¼llerimizi import et
from utils.document_loader import DocumentLoader
from utils.chunking import TextChunker
from utils.embeddings import EmbeddingGenerator
from utils.vector_store import VectorStore

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# Sayfa ayarlarÄ±
st.set_page_config(
    page_title="AkÄ±llÄ± Okuyucu - RAG Chat",
    page_icon="ğŸ“–",
    layout="wide"
)

# OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# Session state'leri initialize et
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None
if 'embedding_generator' not in st.session_state:
    st.session_state.embedding_generator = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'documents_loaded' not in st.session_state:
    st.session_state.documents_loaded = False


def process_uploaded_file(uploaded_file):
    """YÃ¼klenen dosyayÄ± iÅŸle"""
    
    # GeÃ§ici olarak kaydet
    upload_dir = Path("data/uploads")
    upload_dir.mkdir(parents=True, exist_ok=True)
    
    file_path = upload_dir / uploaded_file.name
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    return str(file_path)


def initialize_rag_system():
    """RAG sistemini baÅŸlat"""
    if st.session_state.embedding_generator is None:
        with st.spinner("ğŸ”§ Embedding modeli yÃ¼kleniyor..."):
            st.session_state.embedding_generator = EmbeddingGenerator()
    
    if st.session_state.vector_store is None:
        with st.spinner("ğŸ”§ Vector store baÅŸlatÄ±lÄ±yor..."):
            st.session_state.vector_store = VectorStore(collection_name="rag_documents")


def process_documents(file_paths):
    """DokÃ¼manlarÄ± iÅŸle ve ChromaDB'ye ekle"""
    
    # 1. DokÃ¼manlarÄ± yÃ¼kle
    with st.spinner("ğŸ“„ DokÃ¼manlar okunuyor..."):
        loader = DocumentLoader()
        documents = []
        for file_path in file_paths:
            try:
                doc = loader.load_document(file_path)
                documents.append(doc)
                st.success(f"âœ… {Path(file_path).name} okundu!")
            except Exception as e:
                st.error(f"âŒ {Path(file_path).name} okunamadÄ±: {str(e)}")
    
    if not documents:
        st.error("HiÃ§ dokÃ¼man yÃ¼klenemedi!")
        return False
    
    # 2. Chunking
    with st.spinner("âœ‚ï¸ Metinler bÃ¶lÃ¼nÃ¼yor..."):
        chunker = TextChunker(
            chunk_size=int(os.getenv("CHUNK_SIZE", 500)),
            chunk_overlap=int(os.getenv("CHUNK_OVERLAP", 50))
        )
        chunks = chunker.chunk_documents(documents)
        st.info(f"ğŸ“Š {len(chunks)} chunk oluÅŸturuldu!")
    
    # 3. Embeddings oluÅŸtur
    with st.spinner("ğŸ§® Embeddings oluÅŸturuluyor..."):
        chunks_with_embeddings = st.session_state.embedding_generator.embed_chunks(chunks)
        st.success(f"âœ… {len(chunks_with_embeddings)} embedding hazÄ±r!")
    
    # 4. ChromaDB'ye ekle
    with st.spinner("ğŸ’¾ ChromaDB'ye kaydediliyor..."):
        st.session_state.vector_store.add_chunks(chunks_with_embeddings)
        st.success("âœ… DokÃ¼manlar veritabanÄ±na eklendi!")
    
    return True


def get_rag_response(user_query):
    """RAG ile cevap Ã¼ret"""
    
    # 1. Query'yi embedding'e Ã§evir
    query_embedding = st.session_state.embedding_generator.embed_text(user_query)
    
    # 2. Benzer chunk'larÄ± bul
    results = st.session_state.vector_store.search(query_embedding, top_k=5)
    
    # EÄŸer hiÃ§ benzer chunk yoksa
    if not results:
        return """ÃœzgÃ¼nÃ¼m, yÃ¼klediÄŸiniz dokÃ¼manlarda bu soruyla ilgili bilgi bulamadÄ±m. ğŸ“š
        
Ben **AkÄ±llÄ± Okuyucu**'yum ve sadece yÃ¼klediÄŸiniz PDF veya CSV dosyalarÄ±ndaki bilgileri kullanarak sorularÄ±nÄ±zÄ± cevaplayabilirim.

LÃ¼tfen dokÃ¼manlarÄ±nÄ±zla ilgili sorular sorun! ğŸ˜Š""", []
    
    # 3. Context oluÅŸtur
    context = "\n\n".join([r['text'] for r in results])
    
    # 4. OpenAI ile cevap Ã¼ret
    prompt = f"""Sen AkÄ±llÄ± Okuyucu'sun. Sadece verilen context bilgisini kullanarak soruyu cevapla.

Ã–NEMLÄ° KURALLAR:
1. EÄŸer context'te sorunun cevabÄ± YOKSA, ÅŸunu sÃ¶yle: "Bu bilgi yÃ¼klediÄŸiniz dokÃ¼manlarda bulunmuyor. Ben sadece yÃ¼klediÄŸiniz dokÃ¼manlar hakkÄ±nda bilgi verebiliyorum."
2. ASLA context dÄ±ÅŸÄ±nda bilgi kullanma!
3. EÄŸer soru dokÃ¼manlarla alakasÄ±zsa (Ã¶rn: hava durumu, yemek tarifi, genel bilgi), ÅŸunu sÃ¶yle: "Bu soru yÃ¼klediÄŸiniz dokÃ¼manlarla ilgili deÄŸil. Ben AkÄ±llÄ± Okuyucu'yum ve sadece yÃ¼klediÄŸiniz dosyalardaki bilgileri kullanarak cevap verebilirim."

Context:
{context}

Soru: {user_query}

Cevap:"""
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Sen AkÄ±llÄ± Okuyucu'sun. SADECE verilen context'teki bilgileri kullan. Context dÄ±ÅŸÄ±nda ASLA bilgi verme!"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=500
    )
    
    answer = response.choices[0].message.content
    
    return answer, results


# --- STREAMLIT UI ---

st.title("ğŸ“– AkÄ±llÄ± Okuyucu - RAG Chat UygulamasÄ±")
st.markdown("PDF veya CSV yÃ¼kleyin ve dokÃ¼manlarÄ±nÄ±zla sohbet edin!")

# Sidebar - Dosya YÃ¼kleme
with st.sidebar:
    st.header("ğŸ“ DokÃ¼man YÃ¼kleme")
    
    uploaded_files = st.file_uploader(
        "PDF veya CSV dosyasÄ± yÃ¼kleyin",
        type=['pdf', 'csv'],
        accept_multiple_files=True
    )
    
    if st.button("ğŸš€ DokÃ¼manlarÄ± Ä°ÅŸle", type="primary"):
        if not uploaded_files:
            st.error("LÃ¼tfen Ã¶nce dosya yÃ¼kleyin!")
        else:
            # RAG sistemini baÅŸlat
            initialize_rag_system()
            
            # DosyalarÄ± kaydet
            file_paths = []
            for uploaded_file in uploaded_files:
                file_path = process_uploaded_file(uploaded_file)
                file_paths.append(file_path)
            
            # Ä°ÅŸle
            if process_documents(file_paths):
                st.session_state.documents_loaded = True
                st.balloons()
    
    # Ä°statistikler
    if st.session_state.vector_store:
        st.divider()
        st.subheader("ğŸ“Š Ä°statistikler")
        stats = st.session_state.vector_store.get_stats()
        st.metric("Toplam Chunk", stats['total_documents'])
    
    # VeritabanÄ±nÄ± sÄ±fÄ±rla
    st.divider()
    if st.button("ğŸ—‘ï¸ VeritabanÄ±nÄ± SÄ±fÄ±rla", type="secondary"):
        if st.session_state.vector_store:
            st.session_state.vector_store.delete_collection()
            st.session_state.vector_store = None
            st.session_state.documents_loaded = False
            st.session_state.chat_history = []
            st.success("VeritabanÄ± sÄ±fÄ±rlandÄ±!")
            st.rerun()


# Ana alan - Chat
if not st.session_state.documents_loaded:
    st.info("ğŸ‘ˆ Sol taraftan dokÃ¼man yÃ¼kleyip iÅŸleyin, ardÄ±ndan soru sorabilirsiniz!")
else:
    st.success("âœ… DokÃ¼manlar yÃ¼klendi! Soru sorabilirsiniz.")
    
    # Chat history gÃ¶ster
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # EÄŸer kaynak varsa gÃ¶ster
            if "sources" in message and message["sources"]:
                with st.expander("ğŸ“š Kaynaklar"):
                    for i, source in enumerate(message["sources"], 1):
                        st.markdown(f"**Kaynak {i}** (Benzerlik: {source['distance']:.4f})")
                        st.text(source['text'][:200] + "...")
                        st.json(source['metadata'])
    
    # Chat input
    if prompt := st.chat_input("Sorunuzu yazÄ±n..."):
        # KullanÄ±cÄ± mesajÄ±nÄ± gÃ¶ster
        with st.chat_message("user"):
            st.markdown(prompt)
        
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        
        # RAG cevabÄ± al
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” DÃ¼ÅŸÃ¼nÃ¼yorum..."):
                answer, sources = get_rag_response(prompt)
                st.markdown(answer)
                
                # KaynaklarÄ± gÃ¶ster
                if sources:
                    with st.expander("ğŸ“š Kaynaklar"):
                        for i, source in enumerate(sources, 1):
                            st.markdown(f"**Kaynak {i}** (Benzerlik: {source['distance']:.4f})")
                            st.text(source['text'][:200] + "...")
                            st.json(source['metadata'])
        
        st.session_state.chat_history.append({
            "role": "assistant", 
            "content": answer,
            "sources": sources
        })